% Script to setup and train a convolutional neural network for permeability
% prediction on porescale geometries
% cf. [2] Section 5.3

%import Data
load('TrainCNNDataGeneral.mat', 'TrainingIm', 'TrainingData', 'ValidationData', 'ValidationIm');

% Data augmentation by adding rotated images
TrainingIm = cat(4, TrainingIm, turn(TrainingIm, -pi / 2));
TrainingData = cat(1, TrainingData(:, 1), TrainingData(:, 4));

% Restrict value range of data sets
indexTraining = (TrainingData(1:end, 1) < 10 & TrainingData(1:end, 1) > 10^(-5));
indexValidation = (ValidationData(1:end, 1) < 10 & ValidationData(1:end, 1) > 10^(-5));
TrainingImRestr = (TrainingIm(:, :, 1, indexTraining));
TrainingDataRestr = (TrainingData(indexTraining, 1));

% build neural network by defining layer structure

layers = [; ...
    imageInputLayer([64, 64, 1]); ...
    convolution2dLayer([3, 3], 20, 'Padding', 'same', 'Name', 'conv'); ...
    reluLayer; ...
    batchNormalizationLayer; ...
    maxPooling2dLayer(2, 'Stride', 2); ...
    convolution2dLayer([2, 2], 40, 'Padding', 'same'); ...
    reluLayer; ...
    batchNormalizationLayer; ...
    maxPooling2dLayer(2, 'Stride', 2); ...
    convolution2dLayer([2, 2], 80, 'Padding', 'same'); ...
    reluLayer; ...
    batchNormalizationLayer; ...
    convolution2dLayer([2, 2], 160, 'Padding', 'same'); ...
    reluLayer; ...
    batchNormalizationLayer; ...
    maxPooling2dLayer(2, 'Stride', 2); ...
    convolution2dLayer([2, 2], 320, 'Padding', 'same'); ...
    reluLayer; ...
    batchNormalizationLayer; ...
    fullyConnectedLayer(15); ...
    tanhLayer; ...
    fullyConnectedLayer(1); ...
    regressionLayer];


% set training options
miniBatchSize = 250;
opts = trainingOptions('adam', ...
    'GradientDecayFactor', .9, ...
    'MaxEpochs', 50, ...
    'InitialLearnRate', 0.005, ...
    'MiniBatchSize', miniBatchSize, ...
    'LearnRateSchedule', 'piecewise', ...
    'LearnRateDropFactor', 0.8, ...
    'LearnRateDropPeriod', 10, ...
    'ValidationFrequency', 50, ...
    'ExecutionEnvironment', 'cpu', ...
    'Shuffle', 'every-epoch', ...
    'Plots', 'training-progress', ...
    'Verbose', true, 'ValidationData', {ValidationIm(:, :, 1, indexValidation), 0.9 + (0.1 * log(ValidationData(indexValidation, 1)))});
net = trainNetwork(single(TrainingImRestr), 0.9+0.1*log(TrainingDataRestr), layers, opts);

%% Evaluate training success

% calculate Rsquare value
predicted = exp(10*(net.predict(ValidationIm(:, :, 1, indexValidation)) - 0.9));
val = ValidationData(indexValidation, 1);
Rsquare = 1 - sum((log(predicted)-real(log(val))).^2) / (sum((log(val)-mean(log(val))).^2))

% plot correlation diagram
plot(log(val), log(predicted), '.');
hold on
plot(log(val), log(val), 'LineWidth', 2);
xlabel('log(K_{2,2}) data');
ylabel('log(K_{2,2}) prediction');
set(gca, 'FontSize', 14);
